# -*- coding: utf-8 -*-
"""CycleGAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G1vJZAz2kVjtu2gMpyPMQs9gFgTXO0zI
"""

!pip install torch_snippets torch_summary --quiet
import itertools
from PIL import Image
from torch_snippets import *
from torchvision import transforms
from torchvision.utils import make_grid
from torchsummary import summary

img = Image.open('Dataset/testA/00030.jpg')
img.size

import torch
import torchvision.transforms as transforms
IMAGE_SIZE = 256
device = 'cuda' if torch.cuda.is_available() else 'cpu'
transform = transforms.Compose([
    transforms.Resize(int(IMAGE_SIZE*1.33)),
    transforms.RandomCrop((IMAGE_SIZE,IMAGE_SIZE)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

import glob
from PIL import Image
import torch
from torch.utils.data import DataLoader, Dataset
from random import randint

class Monet2PhotoDataset(Dataset):
    def __init__(self, monet_dir, photo_dir):
        ### Getting the list of images for Monet paintings and photos using glob
        self.monet_images = glob.glob(monet_dir + '/*.jpg') + glob.glob(monet_dir + '/*.png')
        self.photo_images = glob.glob(photo_dir + '/*.jpg') + glob.glob(photo_dir + '/*.png')

    def __getitem__(self, ix):
        # Load the Monet painting
        monet_image = self.monet_images[ix % len(self.monet_images)]
        # Choose a random photo to pair with the Monet painting
        photo_image = self.choose(self.photo_images)

        monet_image = Image.open(monet_image).convert('RGB')
        photo_image = Image.open(photo_image).convert('RGB')
        return monet_image, photo_image

    def __len__(self):
        # Return the maximum length between Monet paintings and photos
        return max(len(self.monet_images), len(self.photo_images))

    def choose(self, images):
        # Randomly select an image from the provided list of images
        return images[randint(0, len(images)-1)]

    def collate_fn(self, batch): # batch function
        # Separate the batch into two lists: Monet paintings and photos
        srcs, trgs = list(zip(*batch))

        # Apply transformations and convert them to tensors
        srcs = torch.cat([transform(img)[None] for img in srcs], 0).to(device).float()
        trgs = torch.cat([transform(img)[None] for img in trgs], 0).to(device).float()

        return srcs.to(device), trgs.to(device)

# Assuming Dataset folder is in the current directory
base_dir = 'Dataset'
trainA_dir = os.path.join(base_dir, 'trainA')
trainB_dir = os.path.join(base_dir, 'trainB')
testA_dir = os.path.join(base_dir, 'testA')
testB_dir = os.path.join(base_dir, 'testB')

# Create the datasets
trn_ds = Monet2PhotoDataset(trainA_dir, trainB_dir)
val_ds = Monet2PhotoDataset(testA_dir, testB_dir)

# Create the DataLoaders
trn_dl = DataLoader(trn_ds, batch_size=1, shuffle=True, collate_fn=trn_ds.collate_fn)
val_dl = DataLoader(val_ds, batch_size=5, shuffle=True, collate_fn=val_ds.collate_fn)

def weights_init_normal(m):
    classname = m.__class__.__name__
    if classname.find("Conv") != -1:
        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)
        if hasattr(m, "bias") and m.bias is not None:
            torch.nn.init.constant_(m.bias.data, 0.0)
    elif classname.find("BatchNorm2d") != -1:
        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)
        torch.nn.init.constant_(m.bias.data, 0.0)

import torch.nn as nn
class ResidualBlock(nn.Module):
    def __init__(self, in_features):
        super(ResidualBlock, self).__init__()

        self.block = nn.Sequential(
            nn.ReflectionPad2d(1),
            nn.Conv2d(in_features, in_features, 3),
            nn.InstanceNorm2d(in_features),
            nn.ReLU(inplace=True),
            nn.ReflectionPad2d(1),
            nn.Conv2d(in_features, in_features, 3),
            nn.InstanceNorm2d(in_features),
        )

    def forward(self, x):
        return x + self.block(x)

class GeneratorResNet(nn.Module):
    def __init__(self, num_residual_blocks=9):
        super(GeneratorResNet, self).__init__()
        out_features = 64
        channels = 3
        model = [
            nn.ReflectionPad2d(3),
            nn.Conv2d(channels, out_features, 7),
            nn.InstanceNorm2d(out_features),
            nn.ReLU(inplace=True),
        ]
        in_features = out_features
        # Downsampling
        for _ in range(2):
            out_features *= 2
            model += [
                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),
                nn.InstanceNorm2d(out_features),
                nn.ReLU(inplace=True),
            ]
            in_features = out_features

        # Residual blocks
        for _ in range(num_residual_blocks):
            model += [ResidualBlock(out_features)]

        # Upsampling
        for _ in range(2):
            out_features //= 2
            model += [
                nn.Upsample(scale_factor=2),
                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),
                nn.InstanceNorm2d(out_features),
                nn.ReLU(inplace=True),
            ]
            in_features = out_features

        # Output layer
        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]
        self.model = nn.Sequential(*model)
        self.apply(weights_init_normal)
    def forward(self, x):
        return self.model(x)

"""GeneratorResNet class using PyTorch constructs a neural network for image generation. It includes downsampling layers, several residual blocks for feature transformation, and upsampling layers to restore the image size. The network starts and ends with convolutional layers, utilizing techniques like instance normalization, reflection padding, and activation functions to stabilize training. The entire model is initialized with specific weights to improve convergence."""

class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()

        channels, height, width = 3, IMAGE_SIZE, IMAGE_SIZE

        def discriminator_block(in_filters, out_filters, normalize=True):
            """Returns downsampling layers of each discriminator block"""
            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]
            if normalize:
                layers.append(nn.InstanceNorm2d(out_filters))
            layers.append(nn.LeakyReLU(0.2, inplace=True))
            return layers

        self.model = nn.Sequential(
            *discriminator_block(channels, 64, normalize=False),
            *discriminator_block(64, 128),
            *discriminator_block(128, 256),
            *discriminator_block(256, 512),
            nn.ZeroPad2d((1, 0, 1, 0)),
            nn.Conv2d(512, 1, 4, padding=1)
        )
        self.apply(weights_init_normal)

    def forward(self, img):
        return self.model(img)

"""The Discriminator class in PyTorch builds a convolutional neural network for image classification in a Generative Adversarial Network (GAN). It uses sequential downsampling blocks with convolutional layers, instance normalization, and LeakyReLU activation to identify real or fake images, followed by a final layer for output. The model is initialized with specified weights."""

@torch.no_grad()
def generate_sample():
    data = next(iter(val_dl))
    G_AB.eval()
    G_BA.eval()

    real_A, real_B = data
    fake_B = G_AB(real_A)
    fake_A = G_BA(real_B)
    # Arange images along x-axis
    real_A = make_grid(real_A, nrow=5, normalize=True)
    real_B = make_grid(real_B, nrow=5, normalize=True)
    fake_A = make_grid(fake_A, nrow=5, normalize=True)
    fake_B = make_grid(fake_B, nrow=5, normalize=True)
    # Arange images along y-axis
    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1)
    show(image_grid.detach().cpu().permute(1,2,0).numpy(), sz=12)

def generator_train_step(Gs, optimizer, real_A, real_B):
    G_AB, G_BA = Gs
    optimizer.zero_grad()
    loss_id_A = criterion_identity(G_BA(real_A), real_A)
    loss_id_B = criterion_identity(G_AB(real_B), real_B)

    loss_identity = (loss_id_A + loss_id_B) / 2
    fake_B = G_AB(real_A)
    loss_GAN_AB = criterion_GAN(D_B(fake_B), torch.Tensor(np.ones((len(real_A), 1, 16, 16))).to(device))
    fake_A = G_BA(real_B)
    loss_GAN_BA = criterion_GAN(D_A(fake_A), torch.Tensor(np.ones((len(real_A), 1, 16, 16))).to(device))

    loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2
    recov_A = G_BA(fake_B)
    loss_cycle_A = criterion_cycle(recov_A, real_A)
    recov_B = G_AB(fake_A)
    loss_cycle_B = criterion_cycle(recov_B, real_B)

    loss_cycle = (loss_cycle_A + loss_cycle_B) / 2
    loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity
    loss_G.backward()
    optimizer.step()
    return loss_G, loss_identity, loss_GAN, loss_cycle, loss_G, fake_A, fake_B

"""The generator_train_step function trains the generator models in a CycleGAN setup, calculating identity, adversarial, and cycle-consistency losses to update the models. It generates fake images, computes losses, performs backpropagation, and updates the generator weights, aiming to enhance image translation quality between two domains."""

def discriminator_train_step(D, real_data, fake_data, optimizer):
    optimizer.zero_grad()
    loss_real = criterion_GAN(D(real_data), torch.Tensor(np.ones((len(real_data), 1, 16, 16))).to(device))
    loss_fake = criterion_GAN(D(fake_data.detach()), torch.Tensor(np.zeros((len(real_data), 1, 16, 16))).to(device))
    loss_D = (loss_real + loss_fake) / 2
    loss_D.backward()
    optimizer.step()
    return loss_D

G_AB = GeneratorResNet().to(device)
G_BA = GeneratorResNet().to(device)
D_A = Discriminator().to(device)
D_B = Discriminator().to(device)

criterion_GAN = torch.nn.MSELoss()
criterion_cycle = torch.nn.L1Loss()
criterion_identity = torch.nn.L1Loss()

optimizer_G = torch.optim.Adam(
    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=0.0002, betas=(0.5, 0.999)
)
optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=0.0002, betas=(0.5, 0.999))
optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=0.0002, betas=(0.5, 0.999))

lambda_cyc, lambda_id = 10.0, 5.0

"""Here the discriminator is updated by minimizing the loss between real and fake data predictions. It computes losses for real and fake images, performs backpropagation, and optimizes the model. The code then defines the generators, discriminators, and their respective optimizers for the CycleGAN setup, using MSE and L1 losses to guide training with specified learning rates and hyperparameters for the adversarial, cycle-consistency, and identity losses."""

# Define the directories to save models
if not os.path.exists('models'):
    os.makedirs('models')

class Report:
    def __init__(self, n_epochs):
        self.n_epochs = n_epochs
        self.logs = []

    def record(self, epoch, **kwargs):
        self.logs.append((epoch, kwargs))
        # Clear the line by printing enough spaces and moving the cursor to the start
        print(f"\r{' ' * 150}", end="")  # Ensure to clear the line completely
        print(f"\rEpoch [{epoch:.2f}] ", end="")  # Move cursor back and start the line correctly
        for key, value in kwargs.items():
            if isinstance(value, (int, float)):
                print(f"{key}: {value:.4f} ", end="")
            else:
                print(f"{key}: {value} ", end="")
        print("", end="\r")  # Keep the carriage return to allow the next overwrite

    def report_avgs(self, epoch):
        avg_log = {key: sum([log[1][key] for log in self.logs if log[0] < epoch]) / len(self.logs) for key in self.logs[0][1].keys()}
        print(f"Epoch [{epoch}/{self.n_epochs}] Averages: ", avg_log)

    def plot_epochs(self, keys):
        """ Plot the losses for the given keys over the epochs. """
        for key in keys:
            values = [log[1][key] for log in self.logs]
            plt.plot(values, label=key)

        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title('Training Losses Over Time')
        plt.legend()
        plt.show()

"""Above code initializes a directory for saving model files if it doesn't already exist. It defines a Report class to manage training logs across epochs. The class features methods to record logs, clear the console line for real-time updates, compute average metrics up to a specified epoch, and plot losses for selected keys over training epochs."""

# Inside the training loop
n_epochs = 35
log = Report(n_epochs)

for epoch in range(n_epochs):
    N = len(trn_dl)
    for bx, batch in enumerate(trn_dl):
        real_A, real_B = batch

        loss_G, loss_identity, loss_GAN, loss_cycle, loss_G, fake_A, fake_B = generator_train_step((G_AB, G_BA), optimizer_G, real_A, real_B)
        loss_D_A = discriminator_train_step(D_A, real_A, fake_A, optimizer_D_A)
        loss_D_B = discriminator_train_step(D_B, real_B, fake_B, optimizer_D_B)
        loss_D = (loss_D_A + loss_D_B) / 2

        # Updated log.record call
        log.record(epoch + (1 + bx) / N,
                   loss_D=loss_D.item(),
                   loss_G=loss_G.item(),
                   loss_GAN=loss_GAN.item(),
                   loss_cycle=loss_cycle.item(),
                   loss_identity=loss_identity.item())
    # Save model after each epoch
    torch.save(G_AB.state_dict(), f'models/G_AB_epoch_{epoch}.pth')
    torch.save(G_BA.state_dict(), f'models/G_BA_epoch_{epoch}.pth')
    torch.save(D_A.state_dict(), f'models/D_A_epoch_{epoch}.pth')
    torch.save(D_B.state_dict(), f'models/D_B_epoch_{epoch}.pth')

    generate_sample()

    log.report_avgs(epoch + 1)

"""The model is trained over 35 epochs, utilizing a Report instance to log metrics. For each epoch, batches of data are processed, where generator and discriminator training steps are executed. The losses from these steps are recorded for real-time monitoring. After completing each epoch, the model states are saved to designated files, and sample outputs are generated."""

import matplotlib.pyplot as plt
log.plot_epochs(['loss_G','loss_GAN', 'loss_D'])

"""Here generator loss, discriminator loss, and GAN loss is visulized with the help of no of epochs in x-axis and loss at y-axis decrease in generator loss and other two losses almost no change."""

import os
import torch
from PIL import Image
from torchvision import transforms

def inference_cycle_gan(input_image_path, output_image_path, generator, direction="A_to_B", device="cuda"):
    # Load the image
    input_image = Image.open(input_image_path)
    input_image_tensor = transforms.ToTensor()(input_image).unsqueeze(0)  # Add batch dimension

    # Move input image to the same device as the generator
    input_image_tensor = input_image_tensor.to(device)

    # Ensure the generator is on the correct device
    generator = generator.to(device)

    # Run the generator (assuming the generator outputs a tensor in [-1, 1] range)
    with torch.no_grad():
        if direction == "A_to_B":
            fake_image = generator(input_image_tensor)[0]
        else:
            fake_image = generator(input_image_tensor)[0]

    # Move the fake image back to CPU and denormalize to [0, 1]
    fake_image = fake_image.cpu()
    fake_image = (fake_image + 1) / 2.0  # Denormalize

    # Convert to PIL image
    fake_image_pil = transforms.ToPILImage()(fake_image)

    # Ensure the output path has a valid extension
    if not os.path.splitext(output_image_path)[1]:
        output_image_path += ".jpg"  # Add default extension if none exists

    # Save the image
    fake_image_pil.save(output_image_path)

# Example call
inference_cycle_gan('Mahesh-Babu-1.jpg', 'output_image', G_AB, direction="A_to_B", device="cuda")

"""Here performing inference using a CycleGAN model. It takes an input image path and generates a transformed output image based on the specified direction like photo to monet or monet to photo. The function loads the image, processes it through the generator, and saves the resulting image to the specified output path. Used a celebrity image for the testing purpose."""

# Example call
inference_cycle_gan('samantha.j', 'output_image', G_AB, direction="A_to_B", device="cuda")

"""Used another celebrity image for testing purpose attached these two images.

CycleGAN is a class of generative adversarial network that allows the model to make a translation of an image to and from other domains with unpaired data. That is, it could take images of paintings  and convert them into photorealistic images. The main idea of CycleGAN involves the use of a cycle-consistency loss in order to make sure the transformation will retain the underlying content of the image, thus enabling creative applications in style transfer and image synthesis. This ability has made it a favorite among artists and researchers trying to fill the gap between different visual styles. In the above code for 35 epochs we can see images if we train it with more epochs we can see good results.
"""

